Most payment card numbers are 16 digits long. The leftmost 6 digits represent a unique identification number for the bank who has issued the card. The next 2 digits determine the type of the card (e.g., debit, credit, gift). Digits 9 to 15 are the serial number of the card, and the last digit is used as a control digit to verify whether the card number is valid. Hence, if somebody enters the card number incorrectly, there is a high chance that a payment software can easily determine it.
Most cards use an algorithm invented by Hans Peter Luhn, a nice fellow from IBM. According to Luhn’s algorithm, you can determine if a credit card number is (syntactically) valid as follows:

1.Double every second digit from right to left. If this “doubling” results in a two-digit number, subtract 9 from it get a single digit.
2.Now add all single digit numbers from step 1.
3.Add all digits in the odd places from right to left in the credit card number.
4.Sum the results from steps 2 & 3.
5.If the result from step 4 is divisible by 10, the card number is valid; otherwise, it is invalid.
for example: